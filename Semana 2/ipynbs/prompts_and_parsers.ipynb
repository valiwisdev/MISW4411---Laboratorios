{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7725174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763208d",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [ Llamadas básicas al modelo](#toc1_)    \n",
    "- [Creación y ejecución del modelo usando LangChain](#toc2_)    \n",
    "- [ Creación de un prompt parametrizado](#toc3_)    \n",
    "- [Estructuración de la salida](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eff82b",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[ Llamadas básicas al modelo](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b117dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configura el modelo utilizando la clave de API almacenada en el entorno\n",
    "genai.configure(api_key = os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Función que realiza una llamada directa al modelo gemini-2.5-flash\n",
    "def get_completion(prompt, model = \"gemini-2.5-flash\"):\n",
    "    \n",
    "    # Se crea una instancia del modelo generativo especificado\n",
    "    chat_model = genai.GenerativeModel(model)\n",
    "    \n",
    "    # Se genera una respuesta con temperatura 0 para minimizar la aleatoriedad\n",
    "    response = chat_model.generate_content(\n",
    "        prompt,\n",
    "        generation_config = {\"temperature\": 0}\n",
    "    )\n",
    "    \n",
    "    # Se retorna únicamente el texto de la respuesta\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f4d7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760848545.959295  153622 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What is the capital of Colombia?\n",
      "Model: The capital of Colombia is **Bogotá**.\n"
     ]
    }
   ],
   "source": [
    "# Se obtiene la respuesta del modelo a partir de un prompt simple\n",
    "response = get_completion(\"What is the capital of Colombia?\")  \n",
    "\n",
    "# Se imprime la interacción de forma simulada entre el usuario y el modelo\n",
    "print(\"\\nUser: What is the capital of Colombia?\")  \n",
    "print(\"Model: \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a6289",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Creación y ejecución del modelo usando LangChain](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd495ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760848550.306096  153622 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Se crea una instancia del modelo gemini-2.5-flash con temperatura 0 (respuestas determinísticas)\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",     \n",
    "    temperature=0.0             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26d1af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification:\n",
      "\n",
      "sentiment: negative\n",
      "urgency: high\n"
     ]
    }
   ],
   "source": [
    "# Se define el mensaje que se enviará al modelo, incluyendo la instrucción, el contenido del mensaje y el formato esperado\n",
    "prompt_customer_1 = \"\"\"\n",
    "Analyze the following message and classify it according to its overall sentiment and level of urgency.\n",
    "\n",
    "Message: ```I placed an order over a week ago and received a product that was completely different from what I purchased. I have sent three emails to customer support and still have not received a response.\n",
    "This is unacceptable. I need an urgent solution or I will escalate the case to the superintendent.```\n",
    "\n",
    "Return your response in the following format:\n",
    "sentiment: <positive | neutral | negative>\n",
    "urgency: <high | medium | low>\n",
    "\"\"\"\n",
    "\n",
    "# Se envía el mensaje al modelo como un objeto HumanMessage y se obtiene la respuesta\n",
    "response = chat.invoke([HumanMessage(content = prompt_customer_1)])\n",
    "\n",
    "# Se imprime únicamente el contenido textual de la respuesta generada por el modelo\n",
    "print(\"\\nClassification:\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df815aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification:\n",
      "\n",
      "sentiment: positive\n",
      "urgency: low\n"
     ]
    }
   ],
   "source": [
    "# Se define el mensaje que se enviará al modelo, incluyendo la instrucción, el contenido del mensaje y el formato esperado\n",
    "prompt_customer_2 = \"\"\"\n",
    "Analyze the following message and classify it according to its overall sentiment and level of urgency.\n",
    "\n",
    "Message: ```Hello! I wanted to congratulate you on the improvements to the website; it is much faster and clearer. Just as a suggestion, you could consider adding a price filter in\n",
    "the products section. It's not urgent, just an idea that I think could be useful.```\n",
    "\n",
    "Return your response in the following format:\n",
    "sentiment: <positive | neutral | negative>\n",
    "urgency: <high | medium | low>\n",
    "\"\"\"\n",
    "\n",
    "# Se envía el mensaje al modelo como un objeto HumanMessage y se obtiene la respuesta\n",
    "response = chat.invoke([HumanMessage(content = prompt_customer_2)])  \n",
    "\n",
    "# Se imprime únicamente el contenido textual de la respuesta generada por el modelo\n",
    "print(\"\\nClassification:\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b36f29",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[ Creación de un prompt parametrizado](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c7317b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Se define una plantilla de prompt con un marcador de posición {message}, que será reemplazado dinámicamente\n",
    "template = \"\"\"\n",
    "Analyze the following message and classify it according to its overall sentiment and level of urgency.\n",
    "\n",
    "Message: ```{message}```\n",
    "\n",
    "Return your response in the following format:\n",
    "sentiment: <positive | neutral | negative>\n",
    "urgency: <high | medium | low>\n",
    "\"\"\"\n",
    "\n",
    "# Se construye un objeto ChatPromptTemplate a partir del texto con marcador de posición\n",
    "prompt_template = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bb91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input variables for the classification prompt:\n",
      "\n",
      "['message']\n"
     ]
    }
   ],
   "source": [
    "# Se imprime la lista de variables de entrada definidas en la plantilla del prompt\n",
    "print(\"\\nInput variables for the classification prompt:\\n\")\n",
    "print(prompt_template.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707a8b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153622/3280055889.py:9: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chat(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification:\n",
      "\n",
      "sentiment: negative\n",
      "urgency: high\n"
     ]
    }
   ],
   "source": [
    "# Se define el primer mensaje de un cliente con tono negativo y urgencia alta\n",
    "customer_1 = \"\"\"I placed an order over a week ago and received a product that was completely different from what I purchased. I have sent three emails to customer support and still have not received a response.\n",
    "This is unacceptable. I need an urgent solution or I will escalate the case to the superintendent.\"\"\"\n",
    "\n",
    "# Se genera el prompt final a partir del template, insertando el mensaje como valor del campo 'message'\n",
    "messages = prompt_template.format_messages(message = customer_1)  \n",
    "\n",
    "# Se envía el prompt generado al modelo y se obtiene la respuesta\n",
    "response = chat(messages)\n",
    "\n",
    "# Se imprime el contenido de la respuesta del modelo\n",
    "print(\"\\nClassification:\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "787c0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification:\n",
      "\n",
      "sentiment: positive\n",
      "urgency: low\n"
     ]
    }
   ],
   "source": [
    "# Se define el segundo mensaje de un cliente con tono positivo y una sugerencia sin urgencia\n",
    "customer_2 = \"\"\"Hello! I wanted to congratulate you on the improvements to the website; it is much faster and clearer. Just as a suggestion, you could consider adding a price filter in\n",
    "the products section. It's not urgent, just an idea that I think could be useful.\"\"\"\n",
    "\n",
    "# Se genera el prompt final a partir del template, insertando el mensaje como valor del campo 'message'\n",
    "messages = prompt_template.format_messages(message = customer_2) \n",
    "\n",
    "# Se envía el prompt generado al modelo y se obtiene la respuesta\n",
    "response = chat(messages)\n",
    "\n",
    "# Se imprime el contenido de la respuesta del modelo\n",
    "print(\"\\nClassification:\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bdc96",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Estructuración de la salida](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c8269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions generated by the parser:\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"sentiment\": string  // Message sentiment\n",
      "\t\"urgency\": string  // Message urgency level\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Se definen los campos que se esperan en la salida del modelo,\n",
    "# cada uno con su nombre y descripción\n",
    "schemas = [\n",
    "    ResponseSchema(name = \"sentiment\", description = \"Message sentiment\"),  # Sentimiento general del mensaje\n",
    "    ResponseSchema(name = \"urgency\", description = \"Message urgency level\")  # Nivel de urgencia del mensaje\n",
    "]\n",
    "\n",
    "# Se construye un parser estructurado a partir del esquema definido\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "# Se genera la instrucción que se debe incluir en el prompt para que el modelo\n",
    "# devuelva una respuesta que cumpla con el formato esperado por el parser\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Se imprime la instrucción de formato que debe incluirse dentro del prompt\n",
    "print(\"\\nInstructions generated by the parser:\\n\")\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3124cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una nueva plantilla de prompt que incluye:\n",
    "# 1. La instrucción general de clasificación.\n",
    "# 2. El mensaje a analizar (variable dinámica: {message}).\n",
    "# 3. Las instrucciones generadas por el parser para estructurar la salida (variable: {format_instructions}).\n",
    "\n",
    "new_template = \"\"\"\n",
    "Analyze the following message and classify it according to its overall sentiment <positive | neutral | negative> and level of urgency <high | medium | low>. \n",
    "\n",
    "Message: ```{message}```\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64531419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response content type (raw):\n",
      "\n",
      "<class 'str'>\n",
      "\n",
      "Parsed object type:\n",
      "\n",
      "<class 'dict'>\n",
      "\n",
      "Value of \"urgency\" key:\n",
      "\n",
      "high\n"
     ]
    }
   ],
   "source": [
    "# Se define el mensaje del cliente a analizar\n",
    "customer_1 = \"\"\"I placed an order over a week ago and received a product that was completely different from what I purchased. I have sent three emails to customer support and still have not received a response.\n",
    "This is unacceptable. I need an urgent solution or I will escalate the case to the superintendent.\"\"\"\n",
    "\n",
    "# Se construye el template dinámico a partir del nuevo prompt con instrucciones estructurales\n",
    "new_prompt_template = ChatPromptTemplate.from_template(new_template)\n",
    "\n",
    "# Se genera el prompt final inyectando el mensaje y las instrucciones del parser\n",
    "messages = new_prompt_template.format_messages(\n",
    "    message = customer_1, \n",
    "    format_instructions = format_instructions \n",
    ")\n",
    "\n",
    "# Se envía el mensaje al modelo y se obtiene la respuesta\n",
    "response = chat(messages)\n",
    "\n",
    "# Se utiliza el parser para convertir la respuesta del modelo en un diccionario estructurado\n",
    "parsed = parser.parse(response.content)\n",
    "\n",
    "# Se imprime el tipo del contenido original de la respuesta (sigue siendo str)\n",
    "print(\"\\nResponse content type (raw):\\n\")\n",
    "print(type(response.content))  # <class 'str'>\n",
    "\n",
    "# Se imprime el tipo del objeto ya parseado (estructurado)\n",
    "print(\"\\nParsed object type:\\n\")\n",
    "print(type(parsed))  # <class 'dict'>\n",
    "\n",
    "# Ahora es posible acceder a los valores mediante sus claves\n",
    "print('\\nValue of \"urgency\" key:\\n')\n",
    "print(parsed['urgency'])  # Ejemplo: 'high'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
