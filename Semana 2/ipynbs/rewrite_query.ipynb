{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf6e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761435268.455832   11828 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model = \"gemini-2.5-flash\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd83fd0",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Zero-shot Query Rewriting](#toc1_)    \n",
    "- [Few-shot Query Rewriting](#toc2_)    \n",
    "- [Sub-queries](#toc3_)    \n",
    "- [Step-back prompt](#toc4_)    \n",
    "- [HyDE](#toc5_)    \n",
    "- [Ejemplo: RAG sin Query Rewriting vs RAG con Query Rewriting](#toc6_)    \n",
    "  - [RAG sin Query Rewriting](#toc6_1_)    \n",
    "  - [Con Query Rewriting](#toc6_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac050a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3d3e96708f47d68fb6f677e8d9d8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bee46c1524426bb71a21adfa41d6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca03fee3ceb45e09a0b45157586a41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea2c8f2a53646df94471af93aae6b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca526742fb34dfb9f1c43ea6e442da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c956ad0ce7fc434d85eece8b009f738b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7da2379703146e1a7e41d2c257e309d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d619747a6ce4e23972f6b6687e3f03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b805cd2ac3d418aaa773bcb729a9e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f3101d917341e9ac6b61ac8fbca3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7be90821754c5fb6875a9bcccf2e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name = embeddings_model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d619e18",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Zero-shot Query Rewriting](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b57f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_rewrite = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question\n",
    "or common synonyms for key words in the question, make sure to return multiple versions\n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Return 3 different versions of the question.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_rewrite),\n",
    "        (\"human\", \"{question}\")\n",
    "        ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e2fa7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1.  What ingredients does this recipe need?\\n2.  What food items are required for this recipe?\\n3.  Which ingredients are used in this recipe?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--a1d4730b-5454-477f-bed5-5c44bbb0a4f0-0', usage_metadata={'input_tokens': 102, 'output_tokens': 35, 'total_tokens': 380, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"question\": \"Which food items does this recipe need?\"\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c86af0",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Few-shot Query Rewriting](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3160acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"How tall is the Eiffel Tower? It looked so high when I was there last year\",\n",
    "        \"answer\": \"What is the height of the Eiffel Tower?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"1 oz is 28 grams, how many cm is 1 inch?\",\n",
    "        \"answer\": \"Convert 1 inch to cm.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the main point of the article? What did the author try to convey?\",\n",
    "        \"answer\": \"What is the main key point of this article?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4bb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_rewrite),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e719f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1.  What ingredients are required for this recipe?\\n2.  What food items are needed for this recipe?\\n3.  Which ingredients does this recipe call for?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--00cb70bb-ea36-49c6-a438-30bc62fed14a-0', usage_metadata={'input_tokens': 185, 'output_tokens': 36, 'total_tokens': 344, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"question\": \"Which food items does this recipe need?\"\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10468b03",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Sub-queries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b79a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_decompose = \"\"\"You are a helpful assistant that generates search queries based on a single input query.\n",
    "\n",
    "Perform query decomposition. Given a user question, break it down into distinct sub questions that\n",
    "you need to answer in order to answer the original question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_decompose),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7368de50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are the sub-questions and search queries to answer your request:\\n\\n**Sub-questions:**\\n\\n1.  What is the most popular programming language used in machine learning?\\n2.  Is that same language the most popular programming language across all domains?\\n\\n**Search Queries:**\\n\\n*   \"most popular programming language machine learning\"\\n*   \"top programming languages for AI\"\\n*   \"most used programming language overall\"\\n*   \"ranking of programming languages by popularity\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--73d23705-0355-4c8e-928e-78ea85fccc42-0', usage_metadata={'input_tokens': 91, 'output_tokens': 100, 'total_tokens': 245, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"question\": \"\"\"Which is the most popular programming language for machine learning and\n",
    "is it the most popular programming language overall?\"\"\"\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566543b",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Step-back prompt](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d814d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_step_back = \"\"\"You are an expert at taking a specific question and extracting a more generic question that gets at\n",
    "the underlying principles needed to answer the specific question.\n",
    "\n",
    "Given a specific user question, write a more generic question that needs to be answered in order to answer the specific question.\n",
    "\n",
    "If you don't recognize a word or acronym to not try to rewrite it.\n",
    "\n",
    "Write concise questions.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_step_back),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d298d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How does one determine the most popular programming language for a given domain?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--0d0d1e8d-3d5f-496d-bb59-fff532eba30f-0', usage_metadata={'input_tokens': 89, 'output_tokens': 14, 'total_tokens': 501, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"question\": \"\"\"Which is the most popular programming language for machine learning?\"\"\"\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1122ea0",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[HyDE](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62df2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_document = \"\"\"\n",
    "Berkson's paradox, also known as Berkson's bias, collider bias, or Berkson's fallacy, is a result in conditional probability\n",
    "and statistics which is often found to be counterintuitive, and hence a veridical paradox. It is a complicating factor arising in\n",
    "statistical tests of proportions. Specifically, it arises when there is an ascertainment bias inherent in a study design. The effect is\n",
    "related to the explaining away phenomenon in Bayesian networks, and conditioning on a collider in graphical models.\n",
    "\n",
    "It is often described in the fields of medical statistics or biostatistics, as in the original description of the problem by Joseph Berkson.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea5b39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_document_emb = embeddings_model.embed_documents([actual_document])\n",
    "\n",
    "system_hyde = \"\"\"You are an expert at using a question to generate a document useful for answering the question.\n",
    "\n",
    "Given a question, generate a paragraph of text that answers the question.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_hyde),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78c8152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Berkson\\'s paradox, also known as Berkson\\'s bias or collider bias, is a conditional probability paradox where two independent (or even positively correlated) events appear to be negatively correlated within a *selected* subpopulation. It arises when the selection into the observed group is dependent on *both* events, specifically when the observed group consists of individuals where at least one of the events has occurred. For instance, if you only observe people admitted to a hospital (the selected group), and admission is more likely if you have either disease A or disease B (or both), you might observe a negative correlation between diseases A and B *within the hospital population*, even if they are independent in the general population. This is because if a patient *doesn\\'t* have disease A, they *must* be more likely to have disease B (to explain their hospital admission), creating an artificial inverse relationship. The paradox highlights how conditioning on a common effect (the \"collider\" variable, like hospital admission) can induce spurious correlations between its causes.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--f703f48f-b209-454e-b751-352a69c7715c-0', usage_metadata={'input_tokens': 45, 'output_tokens': 210, 'total_tokens': 670, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_document = chain.invoke({\n",
    "    \"question\": \"\"\"What does Berkson's paradox consist on?\"\"\"\n",
    "})\n",
    "\n",
    "hypothetical_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e031a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity without HyDE: [[0.86675569]]\n",
      "Similarity with HyDE: [[0.96079076]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "question_embeddings = embeddings_model.embed_documents([\"What does Berkson's paradox consist on?\"])\n",
    "hypothetical_document_emb = embeddings_model.embed_documents([hypothetical_document.content])\n",
    "\n",
    "print(f\"Similarity without HyDE: {cosine_similarity(question_embeddings, actual_document_emb)}\")\n",
    "print(f\"Similarity with HyDE: {cosine_similarity(hypothetical_document_emb, actual_document_emb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181efef",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Ejemplo: RAG sin Query Rewriting vs RAG con Query Rewriting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126498cd",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[RAG sin Query Rewriting](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24cb415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "document_url = \"https://arxiv.org/pdf/2312.10997.pdf\"\n",
    "loader = PyPDFLoader(document_url)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ea199ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap = 40,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "chunk_texts = list(map(lambda d: d.page_content, chunks))\n",
    "embeddings = embeddings_model.embed_documents(chunk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eb73c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "text_embedding_pairs = zip(chunk_texts, embeddings)\n",
    "db = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0a20301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='D. Evaluation Benchmarks and Tools\n",
      "A series of benchmark tests and tools have been proposed\n",
      "to facilitate the evaluation of RAG.These instruments furnish\n",
      "quantitative metrics that not only gauge RAG model perfor-\n",
      "mance but also enhance comprehension of the model’s capabil-\n",
      "ities across various evaluation aspects. Prominent benchmarks\n",
      "such as RGB, RECALL and CRUD [167]–[169] focus on'\n"
     ]
    }
   ],
   "source": [
    "query = \"Which evaluation tools are useful for evaluating a RAG pipeline?\"\n",
    "\n",
    "contexts = db.similarity_search(query, k=1)\n",
    "\n",
    "print(contexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54795f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert at answering questions based on a context extracted from a document. The context extracted from the document is: {context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd07d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The context mentions the following prominent benchmarks that serve as evaluation tools for RAG pipelines:\\n*   RGB\\n*   RECALL\\n*   CRUD', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--1717c3a9-d216-4630-83b6-c19f96c8d785-0', usage_metadata={'input_tokens': 126, 'output_tokens': 30, 'total_tokens': 518, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"context\": '\\n\\n'.join(list(map(lambda c: c.page_content, contexts))),\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e55f30",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[Con Query Rewriting](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50725281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ParaphrasedQuery(BaseModel):\n",
    "    \n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "    paraphrased_query: str = Field(\n",
    "        description = \"A unique paraphrasing of the original question.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fe6baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_rewrite),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([ParaphrasedQuery])\n",
    "query_analyzer = rewrite_prompt | llm_with_tools | PydanticToolsParser(tools = [ParaphrasedQuery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c528e923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ParaphrasedQuery(paraphrased_query='What are some effective evaluation tools for RAG pipelines?'),\n",
       " ParaphrasedQuery(paraphrased_query='What tools can be used to evaluate a Retrieval Augmented Generation (RAG) pipeline?'),\n",
       " ParaphrasedQuery(paraphrased_query='Recommended evaluation frameworks for RAG systems?')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = query_analyzer.invoke({\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "454a8e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1bc98a4c-2e63-4bd2-a875-88a979025cc5', metadata={}, page_content='appraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens 8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. D ISCUSSION AND FUTURE PROSPECTS'),\n",
       " Document(id='123d6a43-eefc-45b9-8654-869f176abc75', metadata={}, page_content='LLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476 , 2023.'),\n",
       " Document(id='1bc98a4c-2e63-4bd2-a875-88a979025cc5', metadata={}, page_content='appraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens 8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. D ISCUSSION AND FUTURE PROSPECTS'),\n",
       " Document(id='7d3e6e3b-eedb-46a9-a93e-9e14dd10ddf8', metadata={}, page_content='D. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD [167]–[169] focus on')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = []\n",
    "for query in queries:\n",
    "    contexts = contexts + db.similarity_search(query.paraphrased_query, k = 1)\n",
    "\n",
    "contexts = contexts + db.similarity_search(\"Which evaluation tools are useful for evaluating a RAG pipeline?\", k =1)\n",
    "\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d59d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert at answering questions based on a context extracted from a document. The context extracted from the document is: {context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32de1e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A robust framework for the systematic evaluation of RAG models is formed by a combination of state-of-the-art automated tools and prominent benchmarks.\\n\\nRecommended evaluation frameworks and tools include:\\n*   **Automated Tools:** RAGAS [164], ARES [165], and TruLens 8. These tools utilize LLMs to assess quality scores.\\n*   **Prominent Benchmarks:** RGB, RECALL, and CRUD [167]–[169].\\n\\nThese instruments provide quantitative metrics to gauge RAG model performance and enhance understanding of their capabilities across various evaluation aspects.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--144f31a4-af0f-42ce-b415-e5e57de20d56-0', usage_metadata={'input_tokens': 474, 'output_tokens': 127, 'total_tokens': 1118, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"context\": '\\n\\n'.join(list(map(lambda c: c.page_content, contexts))),\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
